# National Health Directory Approach and Strategy

## Strategy

### Strategic Landscape

* Health IT is difficult because of a "tangles of arbitrarily added complexity".
* The problems in Health IT tend to be:
  * Data Diversity rather than Data Scale.
  * Complex Clinical Semantics resolved to arbitrary syntaxes results in "faxonomys". Taxonomies that are clinical, but not clinically valid. That still must be conformed to in order to get clinical things done.
  * An Ontology is a structuring of data that leads to otherwise unavailable insights. I call a "faxonomy" is a structuring of data, where the structure generally interferes with the ability to gain insight. Generally a "faxonomy" serves a Political or political purpose, rather than a clinical one.
  * Generally, everything clinical in a medical billing context has all of the downsides of banking data and clinical data, the upsides of neither, and new problems that arise only when they are together.
  * All data that is generated by clinical care that is paid for by an insurance company is "medical billing data". Not just the medical claims themselves, but the EHR records, MRI records and laboratory tests. In the US it is hard to have clinical data that has not been influenced by the need to have the clinical care payed for by a third party.
  * As a result, the only good starting assumption for Health IT is that *nothing* is what it says on the tin.
  * In many cases the "Faxonomies" are closely guarded intellectual property assets (CPT codes most dramatically), and represent low-key "framing conflicts", between CMS/HHS/CDC and the AMA/AHA/NCPDP/WHO/APA and others.
* It is not true that you have to understand all of it to understand any of it.
* But it is true that you have to have a general understanding of most of it to start understanding some of it.
* AI changes everything, including Health IT.
* It is not clear -how- AI changes any part of what we are doing, or if AI will change Health IT differently than the rest of IT. 
* We have a very narrow opportunity window where everything is algined to achieve a good NPD. We must go fast.
* We have limited personelle resources, and we will aquire more personelle resources in an unpredictable trickle
* A trickle of resources, and the delta between Health IT expertise and developer expertise, makes us especially suseptible to communication problems.
* Goodhart's Law applies to everything we do. Altering the provider enumeration approach will cause unintended consequences, and we should be careful when a change in technology approach might implicity be a policy change.
* Despite this, we will need to change multiple fundementally techo-policy issues. Which we will do carefully.
* Modern data ETL engineering does not map well to code review on github or code reviews generally. ETLs are a data flow plus the code that makes them, so considering just the code is not effective.
* Modern AI development is also going to change how code + data flows review work.
* Because code review and development Best practices that are even 6 months old are largely out of date and todays best practices are guaranteed to be out of date in 6 months, we are in the unfortunate position of inventing a code review process that we know will be irrelvant and "wrong" the moment we have decided on something.
* AI-driven development will stabilize but it is not clear when or what that future state will look like.
* The only things we know for sure about development practice for the long term is A. AI will be doing most of the code and B. Some human will still have to be responsible for understanding what the AI did and ensuring that it is correct.
* Github development flows are vital for ensuring that a team of developers are fully in sync. But right now, we do not have a team of developers, which makes the Github flows a substantial overhead for not much payoff.

### Foundational Technology Strategy

* No Proprietary Lock-in: Avoid proprietary technologies, platforms, or services. Not using things like Microsoft SQL are the obvious implications of this. Less obvious is avoiding services like Amazon Secrets, Glue or Lambda, or a testing approach that only works with or through Github. Or fully relying on proprietary portions of otherwise FOSS systems like Docker Desktop.
* Long-Term Sustainability: Plan for a 50-year timeline using mature, widely adopted technologies. Plan in accordance with the [Lindy Effect](https://en.m.wikipedia.org/wiki/Lindy_effect)
* Portability and down-stream instances: Plan for States, NGOs and other interested parties to fork and extend the NPD codebase. This is by design. If NPD is DNS, then this implementation is like 'bind'. Designed to be used at the root and at delegate directories.
* Preference for Old and Stable Tools: Prioritize tools with 10+ years of proven utility. When you think "Parts of Docker are too young" you are on the right track.
* Prefer FOSS tools that have competitive ecosystems. Podman makes Docker safe to use.
* If there's smoke, go and smell it yourself. a Rickover Principle which in data translates to "report extensively on your data, so that you can know when and how it changes". Practically, this means building data reporting capability from the beginning.
* Use the honeycomb approach. Create small software modules that have consistent frameworks to leverage AI (worker bees) to implement complexity only 'within' each honeycomb. CSViper and Plainerflow are my early attempts at encapsulating this approach.
* Batch jobs based on coder skill and context. When possible ensure that developers are working on tasks where they are not learning fundementally new things. 10% learning on a task is good. 90% learningis just guaranteeing that the task will have to be redone. We do not have time to redo tasks for training purposes.
* Because we will avoiding on the job training. Take the time to train developers, once at least 3 but less than 10 people are in the room at the same, conduct skill and context upgrades to get everyone up to speed on general Health IT, FHIR technology as well as NPD specific technology, and policy constraints. Call in favors to get access to training resources. Consider getting people certified on HCISSP, FHIR, AHIMA, or HIMSS certified?


### Honeycomb approach to Health IT complexity

* Health IT complexity tends to be in combinatorics of small weird problems and requirements creating a tangled and unmanaged whole.
* This is exactly the type of complexity that AI coding tools currently struggle with.
* This makes "Detail denial" an especially dangerous mistake. Detail denial is when the "burden of proof" is on anyone to prove that the details matter. The burden of proof needs to go the other way. Details need to be presumed to matter until they are proved not to matter.
* To solve Health IT problems each type of problem needs to be well framed and boxed in, so that an AI can successfully ignore the complexity of the larger system while it works on a single part of the whole.
* Keep the abstraction layers as thin as possible. All abstractions are leaky. But thin ones keep the underlying complexity close to the surface. To put this another way: automations should tend to make coding easier but never to actually 'avoid' coding.

### Do Code+Data Flow Reviews on larger project components

* In place of software oriented code-reviews and pull requests. Use AI to code, use AI to test, use people to confirm that the tests are compreshensive on both the code and data flows. Figure out how to encorporate this into a CI workflow.
* Increase the size and scope of assigned projects to focus on larger pipeline outcomes. Make the "it is finished when" section of tickets more comprehensive. Top level project management tickets should broad in scope (a weeks worth of work for a ticket?). Smaller tickets should be per-repo and oriented towards the smaller tasks that each developer breaks down the work into.
* "Code Reviews" should focus on "running the branch and its tests" as opposed to trying to read the changes on github pull requests. The problems and issues with code should be highlighted in pull requests, but not found there.
* Ensure that coders have up-to-date AI coding tools to ensure that the "code review" stage is stage is still happening, but with AI ahead of human reviews.

### Cross Data Platform Approach

* Universal Execution Contexts: Ensure support in Python, Jupyter Notebooks, Unix CLI, and SAS. Data pipeline options that do not support all of these and all potential future data contexts are non-starters.
* SQL-Centric Processing: Use plain SQL for transformations, interleaved with Python.
  * When it is not possible to simply hand code the SQL steps a 'compile to sql' approach should be taken
  * There are numerous cases where a data transformation must be in R, Pandas or SAS data steps. But this should never be based on mere programmer preference, and the reasons for the "We cannot use SQL" exceptions should be clearly documented.

### Allow Downstream Users to Clone and Run the NPD

* Dual Schema Design: Separate public and private schemas.
  * Private schema should encapsulate all PII on providers
  * Use Faker or something like to create a fake version of private data for down-stream testers
  * Alternatively simply use a LEFT JOIN COALESE approach to queries. NPD should run fun when the PII tables are totally missing.
  * Design the data system to support extensions, at the state-city-county level for regional healthcare policy decorations of the NPD.
* Downloadable Data
  * As Postgresql Data Dumps
  * As CSV file
  * What else? Diffs? Nightly, weekly?

### Compatibility Requirements

* NPPES Backward Compatibility:
  * Reproduce NPPES flat files
  * Maintain entity types, Medicare codes, and NUCC taxonomy
* Claims & Regulatory Continuity:
  * Medicare ACOs
  * Medicaid ACOs
  * QHPs
  * FQHCs
  * Other legacy systems

### FHIR Compliance

* Correct Resource Use: Use FHIR elements per spec (e.g., PractitionerRole).
* Strict Low-level Schema Alignment: Must pass validation against Core and US Core FHIR schemas.
* Less-strict High-Level Schema Alignment: Create a "most of" approach to FAST NPD schema compliance.
* Try and be compatible with other Directory Servers in the wild
* Design Testing infrastructure so that it can be run against Payer Directory servers too.

### Data Quality and Feedback

* Validate When Possible: Fully validate or track confidence levels.
* Track validation as metadata in the schema. 
* User Feedback Channels:
  * Patient-facing UX for data accuracy
  * Token-limited write-back API for structured corrections
  * Support github tickets as initial implementation of data correction. Eventually we will have to abandon this because people will start to add PII accidentally.
 

### Decision-Making Heuristics

* Reliable > Experimental: Use proven tools.
* Ensure backward/future compatibility.
* Prioritize progress over conformance, get things done while honoring the spirit of the law, if not the letter.

## Engineering Process Philosophy

### Git-Based Collaboration

* Use Git/GitHub workflows.
* Clean, small PRs preferred, but large PRs accepted when necessary.
  * Convert the biggest pull requests into separate modules. Honeycomb approach.
* Emphasize clarity and forward momentum.

### Test-Driven Expectations

* Expectations Before APIs: Define tests before implementation.
* Test-Driven Data Models: Built alongside ETLs with validations.
* ETL Resilience by Design:
  * Validate assumptions
  * Break early and loudly on errors

### Anti-Fragile Infrastructure

* Fail Loudly, Recover Quickly: Small, fragile components improve overall system robustness.
* Continuous Feedback Loops: Tests and checks detect degradations early.
  * Leverage lots of different kinds of tests, generally preferring higher level tests that will detect between system failures
  * Specifically leverage data expectation tests to avoid pipeline technical debt
  * Data expectations should make us aware of healthcare ecosystem changes as much as data etc errors. I.e. if all of the pediatricians are suddenly men. This could mean a dramatic change in the healthcare system or a broken import script.

### Legacy Respect: The Joel on Software Principle

* No Clean Slate Rebuilds: Avoid rewrites; understand existing code first.
* Inherited Code is Knowledge: Features represent hard-earned lessons.
* Respect Before Refactor: Prior design is valid until proven otherwise.
* Avoid Reinventing Mistakes: Rewrites risk losing key insights.
* Evolve, Don't Replace: Improve incrementally.

## Engineering references

### Admiral Rickover Books

* [**The Rickover Effect: How One Man Made a Difference** by Theodore Rockwell](https://www.amazon.com/Rickover-Effect-One-Made-Difference/dp/1591140957)
* [**Against the Tide: Rickoverâ€™s Leadership Principles and the Rise of the Nuclear Navy** by Dave Oliver](https://www.amazon.com/Against-Tide-Rickovers-Leadership-Principles/dp/1612514382)
* [**Rickover: The Struggle for Excellence** by Francis Duncan](https://www.amazon.com/Rickover-Struggle-Excellence-Francis-Duncan/dp/161251384X)
* [**Adm. Hyman G. Rickover: Engineer of Power** by Marc Wortman (short biography, often cited)](https://www.amazon.com/Admiral-Hyman-Rickover-Engineers-Power/dp/1612344044)
* [Freds Tech Strategy Blog Tad](https://www.fredtrotter.com/tag/strategy/)
* [Joel on Software](https://www.joelonsoftware.com/) has a good reading list on his homepage.

